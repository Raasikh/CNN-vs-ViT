# -*- coding: utf-8 -*-
"""deeproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FcacpfTx-euoOkyInULwvgHfs3RLOt5W

We will start with CIFAR-10
"""

import torch
import torch.nn as nn
import torch.optim as optim
import time
import torchvision.transforms as transforms
import torchvision

# Data Augmentation and Normalization for CIFAR-10
transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Resize((224, 224)),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))
])

# Load CIFAR-10 dataset
trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)
testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)

# Function to train, validate, and compute test accuracy for models (CNN/ViT)
def train_validate_test(model, trainloader, testloader, optimizer, criterion, epochs=10, is_vit=False, threshold=95.0):
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model.to(device)
    train_acc_history, val_acc_history, test_acc_history = [], [], []
    best_acc = 0
    threshold_epoch = None  # To track the first epoch where validation accuracy exceeds the threshold
    memory_allocated_history = []  # To track memory allocated per epoch

    for epoch in range(epochs):
        model.train()
        running_loss, correct, total = 0.0, 0, 0
        start_time = time.time()

        # Track initial memory usage at the start of each epoch
        if device == 'cuda':
            memory_allocated = torch.cuda.memory_allocated(device)
            memory_reserved = torch.cuda.memory_reserved(device)
            print(f'Epoch {epoch+1}: Initial Memory Allocated: {memory_allocated / (1024 ** 2):.2f} MB, '
                  f'Memory Reserved: {memory_reserved / (1024 ** 2):.2f} MB')

        # Training phase
        for inputs, labels in trainloader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

        train_accuracy = 100 * correct / total
        train_acc_history.append(train_accuracy)
        train_loss = running_loss / len(trainloader)
        train_time = time.time() - start_time

        # Validation phase
        model.eval()
        val_loss, val_correct, val_total = 0.0, 0, 0
        with torch.no_grad():
            for inputs, labels in testloader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                val_loss += loss.item()
                _, predicted = outputs.max(1)
                val_total += labels.size(0)
                val_correct += predicted.eq(labels).sum().item()

        val_accuracy = 100 * val_correct / val_total
        val_acc_history.append(val_accuracy)
        val_loss = val_loss / len(testloader)

        # Track memory usage at the end of the epoch
        if device == 'cuda':
            memory_allocated = torch.cuda.memory_allocated(device)
            memory_reserved = torch.cuda.memory_reserved(device)
            print(f'Epoch {epoch+1}: Final Memory Allocated: {memory_allocated / (1024 ** 2):.2f} MB, '
                  f'Memory Reserved: {memory_reserved / (1024 ** 2):.2f} MB')
            memory_allocated_history.append(memory_allocated / (1024 ** 2))  # Store memory usage in MB

        # Check if validation accuracy reaches or exceeds the threshold
        if val_accuracy >= threshold and threshold_epoch is None:
            threshold_epoch = epoch + 1  # Record the first epoch when the threshold is reached
            print(f'Validation accuracy reached {threshold}% at epoch {threshold_epoch}')

        # Test accuracy calculation
        test_correct, test_total = 0, 0
        with torch.no_grad():
            for inputs, labels in testloader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                _, predicted = outputs.max(1)
                test_total += labels.size(0)
                test_correct += predicted.eq(labels).sum().item()

        test_accuracy = 100 * test_correct / test_total
        test_acc_history.append(test_accuracy)

        # Print epoch results
        print(f'Epoch {epoch+1}/{epochs}, '
              f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, '
              f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%, '
              f'Test Acc: {test_accuracy:.2f}%, Training Time: {train_time:.2f}s')

        # Save the best model
        if val_accuracy > best_acc:
            best_acc = val_accuracy
            torch.save(model.state_dict(), f'best_model_{"vit" if is_vit else "cnn"}.pth')

    # After training, print when the threshold was first reached (if it was)
    if threshold_epoch is not None:
        print(f'Validation accuracy of {threshold}% was first reached at epoch {threshold_epoch}')
    else:
        print(f'Validation accuracy of {threshold}% was not reached during training.')

    return train_acc_history, val_acc_history, test_acc_history, memory_allocated_history

from torchvision.models import resnet18
import torch.optim as optim
import torch.nn as nn

# Load ResNet18 model with pretrained weights
cnn_model = resnet18(weights='IMAGENET1K_V1')

# Freeze all layers except the fully connected layer
for param in cnn_model.parameters():
    param.requires_grad = False

# Replace the final fully connected layer (for 10 classes in CIFAR-10)
cnn_model.fc = nn.Linear(512, 10)

# Only the parameters of the last layer (fully connected) will be optimized
optimizer_cnn = optim.SGD(cnn_model.fc.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)

# Define loss function
criterion = nn.CrossEntropyLoss()

# Now you can train the model, and only the last layer will be updated.
train_acc_cnn, val_acc_cnn, test_acc_cnn, memory_allocated_history = train_validate_test(cnn_model, trainloader, testloader, optimizer_cnn, criterion, epochs=230, is_vit=False, threshold=95.0)

import matplotlib.pyplot as plt

def plot_training_history(train_acc, val_acc, test_acc, model_name="Model"):
    epochs = len(train_acc)  # Total number of epochs

    plt.figure(figsize=(12, 6))

    # Plot Training Accuracy
    plt.subplot(1, 3, 1)
    plt.plot(range(1, epochs + 1), train_acc, label='Training Accuracy')
    plt.title(f'{model_name} - Training Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    # Plot Validation Accuracy
    plt.subplot(1, 3, 2)
    plt.plot(range(1, epochs + 1), val_acc, label='Validation Accuracy', color='orange')
    plt.title(f'{model_name} - Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    # Plot Test Accuracy
    plt.subplot(1, 3, 3)
    plt.plot(range(1, epochs + 1), test_acc, label='Test Accuracy', color='green')
    plt.title(f'{model_name} - Test Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.tight_layout()
    plt.show()

# Plot for CNN
plot_training_history(train_acc_cnn, val_acc_cnn, test_acc_cnn, model_name="CNN")

pip install timm

# prompt: import timm

import timm

from timm import create_model
import torch.optim as optim

# ViT Model
vit_model = create_model('vit_base_patch16_224', pretrained=True, num_classes=10)

# Freeze all layers except the classification head
for param in vit_model.parameters():
    param.requires_grad = False

for param in vit_model.head.parameters():
    param.requires_grad = True

# Optimizer for ViT (fine-tuning only the last layer)
optimizer_vit = optim.AdamW(vit_model.head.parameters(), lr=3e-4, weight_decay=0.01)

# Loss function
criterion = nn.CrossEntropyLoss()

# Train and Validate the ViT
train_acc_vit, val_acc_vit, test_acc_vit, memory_allocated_history = train_validate_test(vit_model, trainloader, testloader, optimizer_vit, criterion, epochs=13, is_vit=True, threshold=95.0)

import matplotlib.pyplot as plt

def plot_training_history(train_acc, val_acc, test_acc, model_name="Model"):
    epochs = len(train_acc)  # Total number of epochs

    plt.figure(figsize=(12, 6))

    # Plot Training Accuracy
    plt.subplot(1, 3, 1)
    plt.plot(range(1, epochs + 1), train_acc, label='Training Accuracy')
    plt.title(f'{model_name} - Training Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    # Plot Validation Accuracy
    plt.subplot(1, 3, 2)
    plt.plot(range(1, epochs + 1), val_acc, label='Validation Accuracy', color='orange')
    plt.title(f'{model_name} - Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    # Plot Test Accuracy
    plt.subplot(1, 3, 3)
    plt.plot(range(1, epochs + 1), test_acc, label='Test Accuracy', color='green')
    plt.title(f'{model_name} - Test Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.tight_layout()
    plt.show()

# Plot for ViT
plot_training_history(train_acc_vit, val_acc_vit, test_acc_vit, model_name="ViT")



"""Now lets do for CIFAR-100"""

import os
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

import torch
import torch.nn as nn
import torch.optim as optim
import time
import torchvision.transforms as transforms
import torchvision

# Data Augmentation and Normalization for CIFAR-100
transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize((0.5071, 0.4865, 0.4409), (0.2673, 0.2564, 0.2761))  # CIFAR-100 mean and std
])

transform_test = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize((0.5071, 0.4865, 0.4409), (0.2673, 0.2564, 0.2761))  # CIFAR-100 mean and std
])

# Load CIFAR-100 dataset
trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)
testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)

# Function to train, validate, and test the model (same as your original)
def train_validate_test(model, trainloader, testloader, optimizer, criterion, epochs=10, is_vit=False, threshold=95.0):
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model.to(device)
    train_acc_history, val_acc_history, test_acc_history = [], [], []
    best_acc = 0
    threshold_epoch = None  # To track the first epoch where validation accuracy exceeds the threshold
    memory_allocated_history = []  # To track memory allocated per epoch

    for epoch in range(epochs):
        model.train()
        running_loss, correct, total = 0.0, 0, 0
        start_time = time.time()

        # Track initial memory usage at the start of each epoch
        if device == 'cuda':
            memory_allocated = torch.cuda.memory_allocated(device)
            memory_reserved = torch.cuda.memory_reserved(device)
            print(f'Epoch {epoch+1}: Initial Memory Allocated: {memory_allocated / (1024 ** 2):.2f} MB, '
                  f'Memory Reserved: {memory_reserved / (1024 ** 2):.2f} MB')

        # Training phase
        for inputs, labels in trainloader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

        train_accuracy = 100 * correct / total
        train_acc_history.append(train_accuracy)
        train_loss = running_loss / len(trainloader)
        train_time = time.time() - start_time

        # Validation phase
        model.eval()
        val_loss, val_correct, val_total = 0.0, 0, 0
        with torch.no_grad():
            for inputs, labels in testloader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                val_loss += loss.item()
                _, predicted = outputs.max(1)
                val_total += labels.size(0)
                val_correct += predicted.eq(labels).sum().item()

        val_accuracy = 100 * val_correct / val_total
        val_acc_history.append(val_accuracy)
        val_loss = val_loss / len(testloader)

        # Track memory usage at the end of the epoch
      if device == 'cuda':
            memory_allocated = torch.cuda.memory_allocated(device)
            memory_reserved = torch.cuda.memory_reserved(device)
            print(f'Epoch {epoch+1}: Final Memory Allocated: {memory_allocated / (1024 ** 2):.2f} MB, '
                  f'Memory Reserved: {memory_reserved / (1024 ** 2):.2f} MB')
            memory_allocated_history.append(memory_allocated / (1024 ** 2))  # Store memory usage in MB

        # Check if validation accuracy reaches or exceeds the threshold
        if val_accuracy >= threshold and threshold_epoch is None:
            threshold_epoch = epoch + 1  # Record the first epoch when the threshold is reached
            print(f'Validation accuracy reached {threshold}% at epoch {threshold_epoch}')

        # Test accuracy calculation
        test_correct, test_total = 0, 0
        with torch.no_grad():
            for inputs, labels in testloader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                _, predicted = outputs.max(1)
                test_total += labels.size(0)
                test_correct += predicted.eq(labels).sum().item()

        test_accuracy = 100 * test_correct / test_total
        test_acc_history.append(test_accuracy)

        # Print epoch results
        print(f'Epoch {epoch+1}/{epochs}, '
              f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, '
              f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%, '
              f'Test Acc: {test_accuracy:.2f}%, Training Time: {train_time:.2f}s')

        # Save the best model
        if val_accuracy > best_acc:
            best_acc = val_accuracy
            torch.save(model.state_dict(), f'best_model_{"vit" if is_vit else "cnn"}.pth')

    # After training, print when the threshold was first reached (if it was)
    if threshold_epoch is not None:
        print(f'Validation accuracy of {threshold}% was first reached at epoch {threshold_epoch}')
    else:
        print(f'Validation accuracy of {threshold}% was not reached during training.')

    return train_acc_history, val_acc_history, test_acc_history, memory_allocated_history

from torchvision.models import resnet18
import torch.optim as optim
import torch.nn as nn

# Load ResNet18 model with pretrained weights
cnn_model = resnet18(weights='IMAGENET1K_V1')

# Freeze all layers except the fully connected layer
for param in cnn_model.parameters():
    param.requires_grad = False

# Replace the final fully connected layer (for 10 classes in CIFAR-10)
cnn_model.fc = nn.Linear(512, 100)

# Only the parameters of the last layer (fully connected) will be optimized
optimizer_cnn = optim.SGD(cnn_model.fc.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)

# Define loss function
criterion = nn.CrossEntropyLoss()


# Now you can train the model, and only the last layer will be updated.
train_acc_cnn, val_acc_cnn, test_acc_cnn, memory_allocated_history = train_validate_test(cnn_model, trainloader, testloader, optimizer_cnn, criterion, epochs=200, is_vit=False, threshold=95.0)

import matplotlib.pyplot as plt

def plot_training_history(train_acc, val_acc, test_acc, model_name="Model"):
    epochs = len(train_acc)  # Total number of epochs

    plt.figure(figsize=(12, 6))

    # Plot Training Accuracy
    plt.subplot(1, 3, 1)
    plt.plot(range(1, epochs + 1), train_acc, label='Training Accuracy')
    plt.title(f'{model_name} - Training Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    # Plot Validation Accuracy
    plt.subplot(1, 3, 2)
    plt.plot(range(1, epochs + 1), val_acc, label='Validation Accuracy', color='orange')
    plt.title(f'{model_name} - Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    # Plot Test Accuracy
    plt.subplot(1, 3, 3)
    plt.plot(range(1, epochs + 1), test_acc, label='Test Accuracy', color='green')
    plt.title(f'{model_name} - Test Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.tight_layout()
    plt.show()

# Plot for CNN
plot_training_history(train_acc_cnn, val_acc_cnn, test_acc_cnn, model_name="CNN")

pip install timm

# Commented out IPython magic to ensure Python compatibility.
# %env CUDA_LAUNCH_BLOCKING=1

from timm import create_model
import torch.optim as optim

# ViT Model
vit_model = create_model('vit_base_patch16_224', pretrained=True, num_classes=100)

# Freeze all layers except the classification head
for param in vit_model.parameters():
    param.requires_grad = False

for param in vit_model.head.parameters():
    param.requires_grad = True

# Optimizer for ViT (fine-tuning only the last layer)
optimizer_vit = optim.AdamW(vit_model.head.parameters(), lr=3e-4, weight_decay=0.01)

# Loss function
criterion = nn.CrossEntropyLoss()

# Train and Validate the ViT
train_acc_vit, val_acc_vit, test_acc_vit, memory_allocated_history = train_validate_test(vit_model, trainloader, testloader, optimizer_vit, criterion, epochs=13, is_vit=True, threshold=95.0)



print(vit_model.head)

import matplotlib.pyplot as plt

def plot_training_history(train_acc, val_acc, test_acc, model_name="Model"):
    epochs = len(train_acc)  # Total number of epochs

    plt.figure(figsize=(12, 6))

    # Plot Training Accuracy
    plt.subplot(1, 3, 1)
    plt.plot(range(1, epochs + 1), train_acc, label='Training Accuracy')
    plt.title(f'{model_name} - Training Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    # Plot Validation Accuracy
    plt.subplot(1, 3, 2)
    plt.plot(range(1, epochs + 1), val_acc, label='Validation Accuracy', color='orange')
    plt.title(f'{model_name} - Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    # Plot Test Accuracy
    plt.subplot(1, 3, 3)
    plt.plot(range(1, epochs + 1), test_acc, label='Test Accuracy', color='green')
    plt.title(f'{model_name} - Test Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.tight_layout()
    plt.show()

# Plot for ViT
plot_training_history(train_acc_vit, val_acc_vit, test_acc_vit, model_name="ViT")





"""Oxford-IIIT pets dataset

"""

from google.colab import drive
import os
import tarfile

# Mount Google Drive
drive.mount('/content/drive')

# Paths to the files in Google Drive
images_tar_path = '/content/drive/MyDrive/images.tar.gz'
annotations_tar_path = '/content/drive/MyDrive/annotations.tar.gz'

# Extract paths
extract_path = '/content/dataset'
os.makedirs(extract_path, exist_ok=True)

# Extract images.tar.gz
with tarfile.open(images_tar_path, 'r:gz') as tar:
    tar.extractall(path=extract_path)

# Extract annotations.tar.gz
with tarfile.open(annotations_tar_path, 'r:gz') as tar:
    tar.extractall(path=extract_path)

print(f"Dataset extracted to: {extract_path}")

import os

# Check the directory structure
for root, dirs, files in os.walk(extract_path):
    print(f"Root: {root}, Dirs: {dirs}, Files: {files[:5]}")  # Print only first 5 files in each directory



import os
import shutil  # Import shutil to use file operations

# Paths
images_path = '/content/dataset/images'
trainval_file = '/content/dataset/annotations/trainval.txt'
test_file = '/content/dataset/annotations/test.txt'
train_dir = '/content/dataset/train'
val_dir = '/content/dataset/val'

# Create train/val directories
os.makedirs(train_dir, exist_ok=True)
os.makedirs(val_dir, exist_ok=True)

# Paths
images_path = '/content/dataset/images'
trainval_file = '/content/dataset/annotations/trainval.txt'
test_file = '/content/dataset/annotations/test.txt'
train_dir = '/content/dataset/train'
val_dir = '/content/dataset/val'

# Create train/val directories
os.makedirs(train_dir, exist_ok=True)
os.makedirs(val_dir, exist_ok=True)

# Function to organize images
def organize_images(file_path, dest_dir):
    with open(file_path, 'r') as f:
        for line in f:
            # Split the line and extract only the first two values
            fields = line.strip().split()
            image_name, class_id = fields[0], fields[1]  # Use only the first two fields
            class_dir = os.path.join(dest_dir, class_id)
            os.makedirs(class_dir, exist_ok=True)
            src_image_path = os.path.join(images_path, f"{image_name}.jpg")
            dest_image_path = os.path.join(class_dir, f"{image_name}.jpg")
            if os.path.exists(src_image_path):  # Move only if the file exists
                shutil.move(src_image_path, dest_image_path)

# Organize training and validation sets
organize_images(trainval_file, train_dir)
organize_images(test_file, val_dir)

print("Images organized successfully!")

from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# Data augmentation and normalization
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Load datasets
train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)
val_dataset = datasets.ImageFolder(root=val_dir, transform=transform)

# Data loaders
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)

print(f"Number of training samples: {len(train_dataset)}")
print(f"Number of validation samples: {len(val_dataset)}")

import time

def train_validate_test(model, train_loader, val_loader, optimizer, criterion, epochs, is_vit, threshold=95.0):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    train_acc_history = []
    val_acc_history = []
    test_acc_history = []
    memory_allocated_history = []
    threshold_epoch = None

    best_val_acc = 0.0

    for epoch in range(epochs):
        start_time = time.time()  # Track the start time of the epoch

        # Training phase
        model.train()
        train_correct, train_total = 0, 0
        running_loss = 0.0

        # Track memory usage at the start of the epoch
        if device.type == 'cuda':
            memory_allocated = torch.cuda.memory_allocated(device)
            memory_reserved = torch.cuda.memory_reserved(device)
            print(f"Epoch {epoch + 1}: Initial Memory Allocated: {memory_allocated / (1024 ** 2):.2f} MB, "
                  f"Memory Reserved: {memory_reserved / (1024 ** 2):.2f} MB")

        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, preds = outputs.max(1)
            train_total += labels.size(0)
            train_correct += preds.eq(labels).sum().item()

        train_acc = 100 * train_correct / train_total
        train_acc_history.append(train_acc)

        # Validation phase
        model.eval()
        val_correct, val_total = 0, 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                _, preds = outputs.max(1)
                val_total += labels.size(0)
                val_correct += preds.eq(labels).sum().item()

        val_acc = 100 * val_correct / val_total
        val_acc_history.append(val_acc)

        # Test phase (using the validation loader as an example)
        test_correct, test_total = 0, 0
        with torch.no_grad():
            for inputs, labels in val_loader:  # Replace with test_loader if separate test set is available
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                _, preds = outputs.max(1)
                test_total += labels.size(0)
                test_correct += preds.eq(labels).sum().item()

        test_acc = 100 * test_correct / test_total
        test_acc_history.append(test_acc)

        # Memory tracking at the end of the epoch
        if device.type == 'cuda':
            memory_allocated = torch.cuda.memory_allocated(device)
            memory_reserved = torch.cuda.memory_reserved(device)
            print(f"Epoch {epoch + 1}: Final Memory Allocated: {memory_allocated / (1024 ** 2):.2f} MB, "
                  f"Memory Reserved: {memory_reserved / (1024 ** 2):.2f} MB")
            memory_allocated_history.append(memory_allocated / (1024 ** 2))

        # Calculate training time
        epoch_time = time.time() - start_time

        # Print epoch results
        print(f"Epoch {epoch + 1}/{epochs}: "
              f"Train Acc: {train_acc:.2f}%, "
              f"Val Acc: {val_acc:.2f}%, "
              f"Test Acc: {test_acc:.2f}%, "
              f"Loss: {running_loss:.4f}, "
              f"Training Time: {epoch_time:.2f}s")

        # Check if validation accuracy exceeds the threshold
        if val_acc >= threshold and threshold_epoch is None:
            threshold_epoch = epoch + 1
            print(f"Threshold of {threshold}% reached at epoch {threshold_epoch}")

        # Save the best model
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), f"best_model_{'vit' if is_vit else 'cnn'}.pth")

    # Print threshold summary
    if threshold_epoch is not None:
        print(f"Validation accuracy of {threshold}% was first reached at epoch {threshold_epoch}")
    else:
        print(f"Validation accuracy of {threshold}% was not reached during training.")

    return train_acc_history, val_acc_history, test_acc_history, memory_allocated_history

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision.models import resnet18

# Load ResNet18 model with pretrained weights
cnn_model = resnet18(weights='IMAGENET1K_V1')

# Freeze all layers except the fully connected layer
for param in cnn_model.parameters():
    param.requires_grad = False

# Replace the final fully connected layer (for 37 classes in Oxford-IIIT Pets)
cnn_model.fc = nn.Linear(512, 37)  # Adjust for 37 classes

# Only the parameters of the last layer (fully connected) will be optimized
optimizer_cnn = optim.SGD(cnn_model.fc.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)

# Define loss function
criterion = nn.CrossEntropyLoss()

# Ensure train_validate_test function is defined (including the torch import inside that script)
# Train the model
train_acc_cnn, val_acc_cnn,test_acc_cnn, memory_allocated_history = train_validate_test(
    cnn_model, train_loader, val_loader, optimizer_cnn, criterion, epochs=200, is_vit=False, threshold=95.0
)

import matplotlib.pyplot as plt

def plot_training_history(train_acc, val_acc,test_acc, model_name="Model"):
    epochs = len(train_acc)  # Total number of epochs

    plt.figure(figsize=(12, 6))

    # Plot Training Accuracy
    plt.subplot(1, 3, 1)
    plt.plot(range(1, epochs + 1), train_acc, label='Training Accuracy', color='blue')
    plt.title(f'{model_name} - Training Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    # Plot Validation Accuracy
    plt.subplot(1, 3, 2)
    plt.plot(range(1, epochs + 1), val_acc, label='Validation Accuracy', color='orange')
    plt.title(f'{model_name} - Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    # Plot Test Accuracy
    plt.subplot(1, 3, 3)
    plt.plot(range(1, epochs + 1), test_acc, label='Test Accuracy', color='green')
    plt.title(f'{model_name} - Test Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.tight_layout()
    plt.show()

# Assuming train_acc_cnn, val_acc_cnn, test_acc_cnn and train_acc_vit, val_acc_vit, test_acc_vit are available

# Plot for CNN
plot_training_history(train_acc_cnn, val_acc_cnn,test_acc_cnn, model_name="CNN")

from timm import create_model

# Load Vision Transformer (ViT) model
vit_model = create_model('vit_base_patch16_224', pretrained=True, num_classes=37)

# Freeze all layers except the classification head
for param in vit_model.parameters():
    param.requires_grad = False

for param in vit_model.head.parameters():
    param.requires_grad = True

# Optimizer and loss function
optimizer_vit = optim.AdamW(vit_model.head.parameters(), lr=3e-4, weight_decay=0.01)
criterion = nn.CrossEntropyLoss()

# Train the ViT
train_acc_vit, val_acc_vit,test_acc_vit,  memory_allocated_history_vit = train_validate_test(
    vit_model, train_loader, val_loader, optimizer_vit, criterion, epochs=12, is_vit=True, threshold=90.0
)

import matplotlib.pyplot as plt

def plot_training_history(train_acc, val_acc, test_acc, model_name="Model"):
    epochs = len(train_acc)  # Total number of epochs

    plt.figure(figsize=(12, 6))

    # Plot Training Accuracy
    plt.subplot(1, 3, 1)
    plt.plot(range(1, epochs + 1), train_acc, label='Training Accuracy', color='blue')
    plt.title(f'{model_name} - Training Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    # Plot Validation Accuracy
    plt.subplot(1, 3, 2)
    plt.plot(range(1, epochs + 1), val_acc, label='Validation Accuracy', color='orange')
    plt.title(f'{model_name} - Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    # Plot Test Accuracy
    plt.subplot(1, 3, 3)
    plt.plot(range(1, epochs + 1), test_acc, label='Test Accuracy', color='green')
    plt.title(f'{model_name} - Test Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.tight_layout()
    plt.show()

# Assuming train_acc_cnn, val_acc_cnn, test_acc_cnn and train_acc_vit, val_acc_vit, test_acc_vit are available



# Plot for ViT
plot_training_history(train_acc_vit, val_acc_vit, test_acc_vit, model_name="ViT")



"""Oxford Flowers-102"""

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

import zipfile
import os

# Define the path to the zip file and extraction directory
zip_file_path = '/content/drive/MyDrive/archive (4).zip'  # Update with your actual path
extract_path = '/content/oxford_flowers_dataset'

# Extract the zip file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print(f"Dataset extracted to: {extract_path}")

# Define paths to train, validation, and test directories
train_dir = os.path.join(extract_path, 'dataset', 'train')
valid_dir = os.path.join(extract_path, 'dataset', 'valid')
test_dir = os.path.join(extract_path, 'dataset', 'test')

print(f"Train Directory: {train_dir}")
print(f"Validation Directory: {valid_dir}")
print(f"Test Directory: {test_dir}")

import os

test_dir = '/content/oxford_flowers_dataset/dataset/test'

# Check the contents of the test directory
print("Contents of test directory:", os.listdir(test_dir))

import os
import shutil

def organize_images_by_filename(test_dir, dest_dir):
    for image_file in os.listdir(test_dir):
        if image_file.endswith(".jpg"):
            # Example logic: extract the class name from the filename
            # Modify this based on your filename pattern
            class_name = image_file.split("_")[0]  # Assuming 'class1_image001.jpg'
            class_dir = os.path.join(dest_dir, class_name)
            os.makedirs(class_dir, exist_ok=True)

            # Move the image to the class-specific folder
            shutil.move(os.path.join(test_dir, image_file), os.path.join(class_dir, image_file))

# Example usage
test_dir = '/content/oxford_flowers_dataset/dataset/test'
dest_dir = '/content/oxford_flowers_dataset/dataset/test_reorganized'
organize_images_by_filename(test_dir, dest_dir)

import os
import json
import shutil
import torch
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

# Define paths
dataset_dir = '/content/oxford_flowers_dataset'
train_dir = os.path.join(extract_path, 'dataset', 'train')
valid_dir = os.path.join(extract_path, 'dataset', 'valid')
test_dir = os.path.join(extract_path, 'dataset', 'test_reorganized')
# Load class names from JSON file
with open(os.path.join(dataset_dir, "cat_to_name.json"), "r") as f:
    class_names = json.load(f)

print("Class Names Loaded:", class_names)

# Verify dataset structure
if not os.path.exists(train_dir) or not os.path.exists(valid_dir) or not os.path.exists(test_dir):
    print("Organizing dataset into train/valid/test...")

    # Define function to reorganize images into subfolders
    def organize_images(src_dir, dest_dir, mapping):
        os.makedirs(dest_dir, exist_ok=True)
        for image_name, class_id in mapping.items():
            class_dir = os.path.join(dest_dir, str(class_id))
            os.makedirs(class_dir, exist_ok=True)
            src_path = os.path.join(src_dir, image_name)
            dest_path = os.path.join(class_dir, image_name)
            if os.path.exists(src_path):
                shutil.move(src_path, dest_path)

    # Example usage: Organize dataset (Update paths as needed)
    # organize_images(raw_test_dir, test_dir, mapping_from_file)

# Define transformations
transform_train = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

transform_valid_test = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Load datasets
train_dataset = datasets.ImageFolder(root=train_dir, transform=transform_train)
valid_dataset = datasets.ImageFolder(root=valid_dir, transform=transform_valid_test)
test_dataset = datasets.ImageFolder(root=test_dir, transform=transform_valid_test)

# Create data loaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Dataset information
print(f"Number of training samples: {len(train_dataset)}")
print(f"Number of validation samples: {len(valid_dataset)}")
print(f"Number of test samples: {len(test_dataset)}")
print(f"Classes: {train_dataset.classes}")

import torch
import time

def train_validate_test(model, train_loader, val_loader, test_loader, optimizer, criterion, epochs, is_vit, threshold=95.0):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # Metrics storage
    train_acc_history = []
    val_acc_history = []
    test_acc_history = []
    memory_allocated_history = []
    threshold_epoch = None

    best_val_acc = 0.0

    for epoch in range(epochs):
        model.train()
        train_correct, train_total = 0, 0
        running_loss = 0.0
        start_time = time.time()

        # Training loop
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, preds = outputs.max(1)
            train_total += labels.size(0)
            train_correct += preds.eq(labels).sum().item()

        train_acc = 100 * train_correct / train_total
        train_acc_history.append(train_acc)

        # Validation loop
        model.eval()
        val_correct, val_total = 0, 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                _, preds = outputs.max(1)
                val_total += labels.size(0)
                val_correct += preds.eq(labels).sum().item()

        val_acc = 100 * val_correct / val_total
        val_acc_history.append(val_acc)

        # Test loop
        test_correct, test_total = 0, 0
        with torch.no_grad():
            for inputs, labels in test_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                _, preds = outputs.max(1)
                test_total += labels.size(0)
                test_correct += preds.eq(labels).sum().item()

        test_acc = 100 * test_correct / test_total
        test_acc_history.append(test_acc)

        # Track memory usage
        if device.type == 'cuda':
            memory_allocated = torch.cuda.memory_allocated(device)
            memory_reserved = torch.cuda.memory_reserved(device)
            memory_allocated_history.append(memory_allocated / (1024 ** 2))  # Convert to MB
            print(f"Epoch {epoch+1}: Memory Allocated: {memory_allocated / (1024 ** 2):.2f} MB, "
                  f"Memory Reserved: {memory_reserved / (1024 ** 2):.2f} MB")

        # Print epoch results
        end_time = time.time()
        print(f"Epoch {epoch + 1}/{epochs}: "
              f"Train Acc: {train_acc:.2f}%, "
              f"Val Acc: {val_acc:.2f}%, "
              f"Test Acc: {test_acc:.2f}%, "
              f"Loss: {running_loss:.4f}, "
              f"Time: {end_time - start_time:.2f}s")

        # Check if validation accuracy exceeds the threshold
        if val_acc >= threshold and threshold_epoch is None:
            threshold_epoch = epoch + 1
            print(f"Threshold of {threshold}% reached at epoch {threshold_epoch}")

        # Save the best model
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), f"best_model_{'vit' if is_vit else 'cnn'}.pth")

    print("Training Completed.")
    return train_acc_history, val_acc_history, test_acc_history, memory_allocated_history

from torchvision.models import resnet18
import torch.nn as nn
import torch.optim as optim

# Load CNN (ResNet18) model
cnn_model = resnet18(weights='IMAGENET1K_V1')

# Freeze all layers except the fully connected layer
for param in cnn_model.parameters():
    param.requires_grad = False

cnn_model.fc = nn.Linear(512, len(train_dataset.classes))  # Adjust for Oxford Flowers classes (102)

# Optimizer and Loss Function
optimizer_cnn = optim.SGD(cnn_model.fc.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)
criterion = nn.CrossEntropyLoss()

# Train CNN
# Train CNN
train_acc_cnn, val_acc_cnn, test_acc_cnn, memory_allocated_history_cnn = train_validate_test(
    cnn_model, train_loader, valid_loader, test_loader, optimizer_cnn, criterion, epochs=200, is_vit=False, threshold=95.0
)

import matplotlib.pyplot as plt

def plot_training_history(train_acc, val_acc, test_acc, model_name="Model"):
    epochs = len(train_acc)  # Total number of epochs

    plt.figure(figsize=(12, 6))

    # Plot Training Accuracy
    plt.subplot(1, 3, 1)
    plt.plot(range(1, epochs + 1), train_acc, label='Training Accuracy', color='blue')
    plt.title(f'{model_name} - Training Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    # Plot Validation Accuracy
    plt.subplot(1, 3, 2)
    plt.plot(range(1, epochs + 1), val_acc, label='Validation Accuracy', color='orange')
    plt.title(f'{model_name} - Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    # Plot Test Accuracy
    plt.subplot(1, 3, 3)
    plt.plot(range(1, epochs + 1), test_acc, label='Test Accuracy', color='green')
    plt.title(f'{model_name} - Test Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.tight_layout()
    plt.show()

# Assuming train_acc_cnn, val_acc_cnn, test_acc_cnn and train_acc_vit, val_acc_vit, test_acc_vit are available



# Plot for ViT
plot_training_history(train_acc_cnn, val_acc_cnn,test_acc_cnn, 'CNN')

pip install timm

from timm import create_model
import torch.nn as nn
import torch.optim as optim
# Load ViT model
vit_model = create_model('vit_base_patch16_224', pretrained=True, num_classes=len(train_dataset.classes))

# Freeze all layers except the head
for param in vit_model.parameters():
    param.requires_grad = False

for param in vit_model.head.parameters():
    param.requires_grad = True

# Optimizer and Loss Function
optimizer_vit = optim.AdamW(vit_model.head.parameters(), lr=3e-4, weight_decay=0.01)
criterion = nn.CrossEntropyLoss()

# Train ViT
train_acc_vit, val_acc_vit, test_acc_vit, memory_allocated_history_vit = train_validate_test(
    vit_model, train_loader, valid_loader,test_loader, optimizer_vit, criterion, epochs=13, is_vit=True, threshold=95.0
)

import matplotlib.pyplot as plt

def plot_training_history(train_acc, val_acc, test_acc, model_name="Model"):
    epochs = len(train_acc)  # Total number of epochs

    plt.figure(figsize=(12, 6))

    # Plot Training Accuracy
    plt.subplot(1, 3, 1)
    plt.plot(range(1, epochs + 1), train_acc, label='Training Accuracy', color='blue')
    plt.title(f'{model_name} - Training Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    # Plot Validation Accuracy
    plt.subplot(1, 3, 2)
    plt.plot(range(1, epochs + 1), val_acc, label='Validation Accuracy', color='orange')
    plt.title(f'{model_name} - Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    # Plot Test Accuracy
    plt.subplot(1, 3, 3)
    plt.plot(range(1, epochs + 1), test_acc, label='Test Accuracy', color='green')
    plt.title(f'{model_name} - Test Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.tight_layout()
    plt.show()

# Assuming train_acc_cnn, val_acc_cnn, test_acc_cnn and train_acc_vit, val_acc_vit, test_acc_vit are available



# Plot for ViT
plot_training_history(train_acc_vit, val_acc_vit,test_acc_vit,'ViT')